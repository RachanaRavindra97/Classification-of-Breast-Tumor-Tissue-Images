# -*- coding: utf-8 -*-
"""Classification of Breast Tumor Tissue Images

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WYKoBK_WrK1G0gRILhK1PuDdSAORxwDw
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from sklearn.utils import resample
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import os 
from os import listdir
from tqdm import tqdm
import shutil

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten
from tensorflow.keras.utils import to_categorical
from keras.preprocessing import image

# %matplotlib inline
#************************************************

#os.mkdir('augmented')
os.system('/content/drive/MyDrive/cs286 project/BreaKHis_v1/histology_slides/breast/benign/Merged')
os.system('/content/drive/MyDrive/cs286 project/BreaKHis_v1/histology_slides/breast/training')

def getListOfFiles(dirName):
    listOfFile = os.listdir(dirName)
    allFiles = list()
    for entry in listOfFile:
        fullPath = os.path.join(dirName, entry)
        if os.path.isdir(fullPath):
            allFiles = allFiles + getListOfFiles(fullPath)
        else:
            allFiles.append(fullPath)
                
    return allFiles

benign_images = getListOfFiles('/content/drive/MyDrive/CS 286 Project/CS 286 Shared/cs286 project/BreaKHis_v1/histology_slides/breast/benign/SOB')
malignant_images = getListOfFiles('/content/drive/MyDrive/CS 286 Project/CS 286 Shared/cs286 project/BreaKHis_v1/histology_slides/breast/malignant/SOB')

from google.colab import drive
drive.mount('/content/drive')

image.load_img(benign_images[3], target_size=(120,120,1), grayscale=False)

image.load_img(malignant_images[3], target_size=(120,120,1), grayscale=False)

total_images = len(benign_images) + len(malignant_images)
total_images

data = pd.DataFrame(index=np.arange(0, len(benign_images)+len(malignant_images)), columns=["image", "target"])
k=0

for c in [0,1]:
        if c==1:
            for m in range(len(benign_images)):
                data.iloc[k]["image"] = benign_images[m]
                data.iloc[k]["target"] = 0
                k += 1
        else:
            for m in range(len(malignant_images)):
                data.iloc[k]["image"] = malignant_images[m]
                data.iloc[k]["target"] = 1
                k += 1

data.head(10)

data.shape

count_data = data["target"].value_counts()
count_data

import seaborn as sns

target = sns.countplot(data["target"])
target.set_xticklabels(['0','1'])
plt.show()

ben_upsampled = resample(data[data['target']==0],n_samples=data[data['target']==1].shape[0], random_state=42)

up_sampled = pd.concat([data[data['target']==1], ben_upsampled])

up_sampled['target'].value_counts()

ben_upsampled.head(10)

up_sampled.shape

train_image = []
y = []

for i in tqdm(range(up_sampled.shape[0])):
    img = image.load_img(up_sampled['image'].iloc[i], target_size=(32,32,1), grayscale=False)
    img = image.img_to_array(img)
    img = img/255
    train_image.append(img)

X = np.array(train_image)
y = up_sampled.iloc[:,-1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)
X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, random_state=42, test_size=0.2 , shuffle=True)

Y_train = to_categorical(y_train, 2)
Y_test = to_categorical(y_test, 2)
Y_val = to_categorical(y_val, 2)

print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

pip install np_utils

from keras.utils.np_utils import to_categorical
Y_train = to_categorical(y_train, 2)
Y_test = to_categorical(y_test, 2)
Y_val = to_categorical(y_val, 2)

print(X_train.shape)
print(X_test.shape)
print(X_val.shape)

print(y_train.shape)
print(y_test.shape)
print(y_val.shape)

model = Sequential()
#convlouton layer with the number of filters, filter size, strides steps, padding or no, activation type and the input shape.
model.add(Conv2D(30, kernel_size = (3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,3)))
#pooling layer to reduce the volume of input image after convolution,
model.add(MaxPool2D(pool_size=(1,1)))
#flatten layer to flatten the output
model.add(Flatten())   # flatten output of conv
model.add(Dense(150, activation='relu'))  # hidden layer of 150 neuron
model.add(Dense(2, activation='softmax'))  # output layer
model.compile(loss='categorical_crossentropy', metrics= ['accuracy'], optimizer='adam')
history = model.fit(X_train, Y_train, batch_size=20, epochs = 20, validation_data=(X_test, Y_test))

history_df = pd.DataFrame(history.history)
history_df.plot()

from keras.models import Sequential
from keras.layers import Dense

y_pred = model.predict(X_val)
classes_x = np.argmax(y_pred,axis=1)
acc_test = 0

for i in range(X_val.shape[0]):
    if(classes_x[i] == y_val[i]):
        acc_test= acc_test+1
print("Accuracy test : "  , acc_test/X_val.shape[0]*100)

y_pred = model.predict(X_test)
classes_x = np.argmax(y_pred,axis=1)
acc_test = 0

for i in range(X_test.shape[0]):
    if(classes_x[i] == y_test[i]):
        acc_test= acc_test+1
print("Accuracy test : "  , acc_test/X_test.shape[0]*100)





"""# Transfer Learning

# ResNet
"""

def build_model(backbone, lr=1e-4):
    model = Sequential()
    model.add(backbone)
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dropout(0.5))
    model.add(layers.BatchNormalization())
    model.add(layers.Dense(2, activation='softmax'))
    
    
    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(lr=lr),
        metrics=['accuracy']
    )
    
    return model

# Commented out IPython magic to ensure Python compatibility.
import json
import math
import os

import cv2
from PIL import Image
import numpy as np
from keras import layers
from tensorflow.keras.applications import ResNet50,MobileNet, DenseNet201, InceptionV3, NASNetLarge, InceptionResNetV2, NASNetMobile
from keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, TensorBoard
from keras.preprocessing.image import ImageDataGenerator
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score, accuracy_score
import scipy
from tqdm import tqdm
import tensorflow as tf
from keras import backend as K
import gc
from functools import partial
from sklearn import metrics
from collections import Counter
import json
import itertools


# %matplotlib inline

from keras import backend as K
import gc

K.clear_session()
gc.collect()

resnet = DenseNet201(
    weights='imagenet',
    include_top=False,
    input_shape=(32,32,3)
)

model = build_model(resnet ,lr = 1e-4)
model.summary()

# Learning Rate Reducer
learn_control = ReduceLROnPlateau(monitor='val_acc', patience=5,
                                  verbose=1,factor=0.2, min_lr=1e-7)

# Checkpoint
filepath="C:/Users/Checkout/Documents/CS 286 Project/weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')

BATCH_SIZE = 16

# Using original generator
train_generator = ImageDataGenerator(
        zoom_range=2,  # set range for random zoom
        rotation_range = 90,
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True,  # randomly flip images
    )

history = model.fit_generator(
    train_generator.flow(X_train, Y_train, batch_size=BATCH_SIZE),
    steps_per_epoch=X_train.shape[0] / BATCH_SIZE,
    epochs=20,
    validation_data=(X_val, Y_val),
    callbacks=[learn_control, checkpoint]
)

history_df = pd.DataFrame(history.history)
history_df[['accuracy', 'val_accuracy']].plot()



history_df = pd.DataFrame(history.history)
history_df[['loss', 'val_loss']].plot()

#model.load_weights("C:/Users/Checkout/Documents/CS 286 Project/weights.best.hdf5")
Y_val_pred = model.predict(X_val)
accuracy_score(np.argmax(Y_val, axis=1), np.argmax(Y_val_pred, axis=1))

Y_pred = model.predict(X_test)
tta_steps = 10
predictions = []

for i in tqdm(range(tta_steps)):
    preds = model.predict_generator(train_generator.flow(X_test, batch_size=BATCH_SIZE, shuffle=False),
                                    steps = len(X_test)/BATCH_SIZE)
    
    predictions.append(preds)
    gc.collect()
    
Y_pred_tta = np.mean(predictions, axis=0)

















